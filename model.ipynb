{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "\tdef __init__(self, numChannels, classes):\n",
    "\t\t# call the parent constructor\n",
    "\t\tsuper(LeNet, self).__init__()\n",
    "\n",
    "\t\t# initialize first set of CONV => RELU => POOL layers\n",
    "\t\tself.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\n",
    "\t\t\tkernel_size=(5, 5))\n",
    "\t\tself.relu1 = ReLU()\n",
    "\t\tself.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\t# initialize second set of CONV => RELU => POOL layers\n",
    "\t\tself.conv2 = Conv2d(in_channels=20, out_channels=30,\n",
    "\t\t\tkernel_size=(5, 5))\n",
    "\t\tself.relu2 = ReLU()\n",
    "\t\tself.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\tself.conv3 = Conv2d(in_channels=30, out_channels=40,\n",
    "\t\t\tkernel_size=(5, 5))\n",
    "\t\tself.relu3 = ReLU()\n",
    "\t\tself.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\tself.conv4 = Conv2d(in_channels=40, out_channels=50,\n",
    "\t\t\tkernel_size=(5, 5))\n",
    "\t\tself.relu4 = ReLU()\n",
    "\t\tself.maxpool4 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\tself.conv5 = Conv2d(in_channels=50, out_channels=60,\n",
    "\t\t\tkernel_size=(5, 5))\n",
    "\t\tself.relu5 = ReLU()\n",
    "\t\tself.maxpool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\t# initialize first (and only) set of FC => RELU layers\n",
    "\t\tself.fc1 = Linear(in_features=900, out_features=500)\n",
    "\t\tself.relu6 = ReLU()\n",
    "\n",
    "\t\t# initialize our softmax classifier\n",
    "\t\tself.fc2 = Linear(in_features=500, out_features=classes)\n",
    "\t\tself.logSoftmax = LogSoftmax(dim=1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# pass the input through our first set of CONV => RELU =>\n",
    "\t\t# POOL layers\n",
    "\t\tx = self.conv1(x)\n",
    "\t\t#print(f\"shape after first conv: {x.shape}\")\n",
    "\t\tx = self.relu1(x)\n",
    "\t\tx = self.maxpool1(x)\n",
    "\t\t#print(f\"shape after first maxpool: {x.shape}\")\n",
    "\t\t# pass the output from the previous layer through the second\n",
    "\t\t# set of CONV => RELU => POOL layers\n",
    "\t\tx = self.conv2(x)\n",
    "\t\t#print(f\"shape after second conv: {x.shape}\")\n",
    "\t\tx = self.relu2(x)\n",
    "\t\tx = self.maxpool2(x)\n",
    "\t\t#print(f\"shape after second maxpool: {x.shape}\")\n",
    "\n",
    "\t\tx = self.conv3(x)\n",
    "\t\t#print(f\"shape after third conv: {x.shape}\")\n",
    "\t\tx = self.relu3(x)\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\t#print(f\"shape after third maxpool: {x.shape}\")\n",
    "\n",
    "\t\tx = self.conv4(x)\n",
    "\t\t#print(f\"shape after fourth conv: {x.shape}\")\n",
    "\t\tx = self.relu4(x)\n",
    "\t\tx = self.maxpool4(x)\n",
    "\t\t#print(f\"shape after fourth maxpool: {x.shape}\")\n",
    "\n",
    "\t\tx = self.conv5(x)\n",
    "\t\t#print(f\"shape after fifth conv: {x.shape}\")\n",
    "\t\tx = self.relu5(x)\n",
    "\t\tx = self.maxpool5(x)\n",
    "\t\t#print(f\"shape after fifth maxpool: {x.shape}\")\n",
    "\t\t# flatten the output from the previous layer and pass it\n",
    "\t\t# through our only set of FC => RELU layers\n",
    "\t\tx = flatten(x, 1)\n",
    "\t\t#print(f\"shape after flatten: {x.shape}\")\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu6(x)\n",
    "\t\t# pass the output to our softmax classifier to get our output\n",
    "\t\t# predictions\n",
    "\t\tx = self.fc2(x)\n",
    "\t\toutput = self.logSoftmax(x)\n",
    "\t\t# return the output predictions\n",
    "\t\treturn output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, x, y, img_dir):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\tself.img_dir = img_dir\n",
    "\t\tself.classes = np.unique(self.y)\n",
    "\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = os.path.join(self.img_dir, self.x[idx])\n",
    "\t\timage = np.load(img_path)\n",
    "\t\tlabel = self.y[idx]\n",
    "\t\treturn image, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "AUDIO_PATH = '../birdclef-2022-data/train_audio'\n",
    "IMAGE_PATH = '../birdclef-2022-data/train_images/'\n",
    "ANNOTATED_PATH = './data/annotated_data.csv'\n",
    "ONLY_FIRST = True\n",
    "\n",
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                file_name  label\n",
      "0  afrsil1/XC125458_0.npy      0\n",
      "1  afrsil1/XC175522_0.npy      0\n",
      "2  afrsil1/XC177993_0.npy      0\n",
      "3  afrsil1/XC205893_0.npy      0\n",
      "4  afrsil1/XC207431_0.npy      0\n"
     ]
    }
   ],
   "source": [
    "# collect data about data structure\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "all_data = []\n",
    "for primary_label in os.listdir(IMAGE_PATH):\n",
    "\tif ONLY_FIRST:\n",
    "\t\tall_data += [primary_label + '/' +\n",
    "\t\t\t\t\t x for x in os.listdir(IMAGE_PATH + primary_label) if x.endswith('_0.npy')\n",
    "\t\t\t\t\t ]\n",
    "\telse:\n",
    "\t\tall_data += [primary_label + '/' + x for x in os.listdir(IMAGE_PATH + primary_label)]\n",
    "\n",
    "if ONLY_FIRST:\n",
    "\tall_data += ['maupar/XC123887_1.npy']\n",
    "\n",
    "base_data = {'file_name': [], 'label': []}\n",
    "for item in all_data:\n",
    "    base_data['file_name'].append(item)\n",
    "    base_data['label'].append(item.split('/')[0])\n",
    "\n",
    "results = pd.DataFrame(base_data, columns = ['file_name', 'label'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(results['label'])\n",
    "results['label'] = labels\n",
    "print(results.head())\n",
    "if ONLY_FIRST:\n",
    "\tresults.to_csv(\"./data/annotated_data_only_first.csv\", index=False)\n",
    "\tANNOTATED_PATH = './data/annotated_data_only_first.csv'\n",
    "else:\n",
    "\tresults.to_csv(\"./data/annotated_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "                   file_name  label\n",
      "8721   maupar/XC123887_0.npy     90\n",
      "14852  maupar/XC123887_1.npy     90\n"
     ]
    }
   ],
   "source": [
    "data_set = pd.read_csv(ANNOTATED_PATH)\n",
    "values, counts = np.unique(data_set['label'], return_counts=True)\n",
    "print(values[np.argmin(counts)])\n",
    "print(data_set[data_set['label'] == 90])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(ANNOTATED_PATH)\n",
    "y_labels = data_set['label']\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_set['file_name'].to_numpy(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  data_set['label'].to_numpy(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  test_size=0.25,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  stratify=y_labels,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(x_train, y_train, IMAGE_PATH)\n",
    "val_dataset = CustomDataset(x_val, y_val, IMAGE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "# load data\n",
    "# numTrainSamples = round(len(all_data) * TRAIN_SPLIT)\n",
    "# numValSamples = round(len(all_data) * VAL_SPLIT)\n",
    "#\n",
    "# data_set = CustomDataset(\"annotated_data.csv\", IMAGE_PATH)\n",
    "#\n",
    "# (trainData, valData) = random_split(data_set, [numTrainSamples, numValSamples],generator=torch.Generator().manual_seed(42))\n",
    "#\n",
    "# print(f\"len training: {len(trainData)}, len test: {len(valData)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the LeNet model...\n"
     ]
    }
   ],
   "source": [
    "# initialize the LeNet model\n",
    "print(\"[INFO] initializing the LeNet model...\")\n",
    "model = LeNet(\n",
    "\tnumChannels=3,\n",
    "\tclasses=len(train_dataset.classes)).to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()\n",
    "# initialize a dictionary to store training history\n",
    "class_acc = []\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": [],\n",
    "\t\"f1score\": []\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n",
      "[INFO] starting training for epoch: 1 at 15:03:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  15%|█▌        | 27/175 [00:07<00:41,  3.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [314]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# add the loss to the total training loss so far and\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# calculate the number of correct predictions\u001B[39;00m\n\u001B[0;32m     37\u001B[0m totalTrainLoss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m---> 38\u001B[0m trainCorrect \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# add predictions and targets for F1score tracking\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m pred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "\tprint(f\"[INFO] starting training for epoch: {e + 1} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\tpreds = []\n",
    "\ttarget = []\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor data in tqdm(trainDataLoader, desc=\"training progress\"):\n",
    "\t\t# send the input to the device\n",
    "\t\tx, y = data\n",
    "\t\tx = x.transpose(1, 3)\n",
    "\t\tx = x.transpose(2, 3)\n",
    "\t\tx = x.float()\n",
    "\t\tx, y = (x.to(device), y.to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFn(pred, y)\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\t\t# add predictions and targets for F1score tracking\n",
    "\t\tfor item in pred:\n",
    "\t\t\tpreds.append(np.argmax(item.cpu().detach().numpy()))\n",
    "\t\ttarget.extend(y.cpu().detach().numpy())\n",
    "\n",
    "\t# switch off autograd for evaluation\n",
    "\twith torch.no_grad():\n",
    "\t\tprint(f\"[INFO] starting evaluation for epoch: {e + 1} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in tqdm(valDataLoader, desc=\"validation progress\"):\n",
    "\t\t\tx = x.transpose(1, 3)\n",
    "\t\t\tx = x.transpose(2, 3)\n",
    "\t\t\tx = x.float()\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotalValLoss += lossFn(pred, y)\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
    "\tvalCorrect = valCorrect / len(valDataLoader.dataset)\n",
    "\n",
    "\tmatrix = confusion_matrix(target, preds)\n",
    "\tclass_acc.append(matrix.diagonal()/matrix.sum(axis=1))\n",
    "\n",
    "\tf1epoch = f1_score(target, preds, average='macro')\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect)\n",
    "\tH['f1score'].append(f1epoch)\n",
    "\t# print the model training and validation information\n",
    "\tprint(f\"\"\"\n",
    "\t[INFO] EPOCH: {e + 1}/{EPOCHS}\n",
    "\tTrain loss: {avgTrainLoss :.6f}, Train accuracy: {trainCorrect :.4f}\n",
    "\tVal loss: \t{avgValLoss :.6f}, Val accuracy:   {valCorrect :.4f}\n",
    "\tF1: \t\t{f1epoch :.6f}\n",
    "\t---------------------------------------------------- \\n\n",
    "\t\"\"\")\n",
    "\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(f\"[INFO] total time taken to train the model: {(endTime - startTime) / 60 :.2f}min\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_src = \"../birdclef-2022-data\"\n",
    "with open(f\"{data_src}/scored_birds.json\") as f:\n",
    "    scored = json.load(f)\n",
    "\n",
    "scored = le.transform(scored)\n",
    "acc_scored = [class_acc[-1][x] for x in scored]\n",
    "with open(f\"{data_src}/scored_birds.json\") as f:\n",
    "    scored = json.load(f)\n",
    "\n",
    "output = f\"\"\"\n",
    "last epoch values:\n",
    "train loss: {H[\"train_loss\"][-1]}\n",
    "train acc: \t{H[\"train_acc\"][-1]}\n",
    "val loss:\t{H[\"val_loss\"][-1]}\n",
    "val acc: \t{H[\"val_acc\"][-1]}\n",
    "F1: \t\t{H['f1score'][-1]}\n",
    "\"\"\"\n",
    "print(output)\n",
    "print(f\"per class accuracy after last epoch: \\n {class_acc[-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(class_acc[-1])\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams['font.size'] = 20\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18,40))\n",
    "sns.barplot(x=class_acc[-1], y=os.listdir(IMAGE_PATH), ax = ax[0])\n",
    "sns.barplot(x=acc_scored, y=scored, ax = ax[1], dodge=False)\n",
    "ax[0].set_title(\"per class accuracy\")\n",
    "ax[1].set_title(\"accuracy for scored classes\")\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.plot(H[\"f1score\"], label=\"f1_score\")\n",
    "plt.title(\"Training Loss, Accuracy and F1 on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy/F1\")\n",
    "plt.legend(loc=\"lower left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}